{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtN8zTfL8+mjiUvKD1l6ey",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faNa-ml/CNN/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lji1RueGWpJO",
        "outputId": "52bd3bd6-35fb-405c-e475-f9fddce3e661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing and upgrading necessary libraries...\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Libraries installed/upgraded successfully.\n",
            "No GPU found, using CPU.\n",
            "\n",
            "--- Downloading and preparing dataset (handling nested zips) ---\n",
            "Downloading dataset from Majeed Wani, Insha ; Arora, Sakshi (2021), “Knee X-ray Osteoporosis Database”, Mendeley Data, V2, doi: 10.17632/fxjm8fb6mw.2...\n",
            "--2025-06-07 08:39:54--  ftp://majeed%20wani,%20insha%20/;%20Arora,%20Sakshi%20(2021),%20%E2%80%9CKnee%20X-ray%20Osteoporosis%20Database%E2%80%9D,%20Mendeley%20Data,%20V2,%20doi/%2010.17632/fxjm8fb6mw.2\n",
            "           => ‘.listing’\n",
            "Resolving majeed wani, insha  (majeed wani, insha )... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘majeed wani, insha ’\n",
            "Downloaded Knee-X-ray_outer.zip successfully.\n",
            "Extracting Knee-X-ray_outer.zip to /content/first_extracted_zip_contents...\n",
            "Error: Knee-X-ray_outer.zip is not a valid zip file or is corrupted. Attempting to proceed assuming it's an empty or problematic zip, check manually.\n",
            "No inner zip file found in /content/first_extracted_zip_contents. Assuming the first extraction contained the data directly.\n",
            "Copying contents from /content/first_extracted_zip_contents to /content/final_dataset_extracted...\n",
            "Contents copied successfully.\n",
            "Error: Could not find 'normal', 'osteopenia', 'osteoporosis' directories within /content/final_dataset_extracted.\n",
            "\n",
            "--- Contents of FINAL_DATA_EXTRACT_DIR ---\n",
            "/content/final_dataset_extracted:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Class directories not found. Please verify the dataset structure after extraction.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9612dbf38039>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Contents of FINAL_DATA_EXTRACT_DIR ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls -R {FINAL_DATA_EXTRACT_DIR}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Class directories not found. Please verify the dataset structure after extraction.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Class directories not found. Please verify the dataset structure after extraction."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shutil # برای مدیریت فایل‌ها (ساخت و حذف پوشه‌ها)\n",
        "import zipfile # برای کار با فایل‌های زیپ\n",
        "from tqdm import tqdm # برای نمایش نوار پیشرفت در هنگام کپی فایل‌ها\n",
        "\n",
        "# --- ۰. نصب و به‌روزرسانی کتابخانه‌ها (مخصوص Colab) ---\n",
        "print(\"Installing and upgrading necessary libraries...\")\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install scikit-learn pandas matplotlib tqdm\n",
        "print(\"Libraries installed/upgraded successfully.\")\n",
        "\n",
        "# --- ۱. تنظیمات اولیه و بررسی دسترسی به GPU ---\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(f\"Using {len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found, using CPU.\")\n",
        "\n",
        "# --- ۲. دانلود و آماده‌سازی دیتاست در Colab (با مدیریت Double-Zipping) ---\n",
        "print(\"\\n--- Downloading and preparing dataset (handling nested zips) ---\")\n",
        "DATASET_URL = \"Majeed Wani, Insha ; Arora, Sakshi (2021), “Knee X-ray Osteoporosis Database”, Mendeley Data, V2, doi: 10.17632/fxjm8fb6mw.2\"\n",
        "OUTER_ZIP_FILE_NAME = \"Knee-X-ray_outer.zip\"\n",
        "FIRST_EXTRACT_DIR = \"/content/first_extracted_zip_contents\" # مسیری برای اولین استخراج\n",
        "\n",
        "# دانلود فایل زیپ بیرونی\n",
        "print(f\"Downloading dataset from {DATASET_URL}...\")\n",
        "# اگر لینک دانلود مجددا خراب شد، ممکن است نیاز به لینک جدید باشد یا آپلود دستی.\n",
        "!wget -O {OUTER_ZIP_FILE_NAME} \"{DATASET_URL}\"\n",
        "print(f\"Downloaded {OUTER_ZIP_FILE_NAME} successfully.\")\n",
        "\n",
        "# ایجاد دایرکتوری برای اولین استخراج\n",
        "os.makedirs(FIRST_EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "# استخراج فایل زیپ بیرونی\n",
        "print(f\"Extracting {OUTER_ZIP_FILE_NAME} to {FIRST_EXTRACT_DIR}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(OUTER_ZIP_FILE_NAME, 'r') as zip_ref:\n",
        "        zip_ref.extractall(FIRST_EXTRACT_DIR)\n",
        "    print(\"First level extraction complete.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: {OUTER_ZIP_FILE_NAME} is not a valid zip file or is corrupted. Attempting to proceed assuming it's an empty or problematic zip, check manually.\")\n",
        "    # در این حالت، ممکن است zipfile.BadZipFile رخ دهد اما wget فایل را دانلود کرده باشد.\n",
        "    # باید مطمئن شویم که یا یک فایل زیپ داخلی هست یا خطا جدی‌تر است.\n",
        "    # فعلا اجازه می‌دهیم کد ادامه یابد تا شاید زیپ داخلی را پیدا کند.\n",
        "\n",
        "\n",
        "# --- جستجو و استخراج فایل زیپ داخلی ---\n",
        "INNER_ZIP_FILE_PATH = None\n",
        "# جستجو در پوشه استخراج شده اول برای یافتن فایل زیپ داخلی\n",
        "for root, dirs, files in os.walk(FIRST_EXTRACT_DIR):\n",
        "    for file in files:\n",
        "        if file.lower().endswith('.zip'):\n",
        "            INNER_ZIP_FILE_PATH = os.path.join(root, file)\n",
        "            break\n",
        "    if INNER_ZIP_FILE_PATH:\n",
        "        break\n",
        "\n",
        "FINAL_DATA_EXTRACT_DIR = \"/content/final_dataset_extracted\" # مسیر نهایی برای استخراج دیتاست واقعی\n",
        "os.makedirs(FINAL_DATA_EXTRACT_DIR, exist_ok=True) # اطمینان از وجود این پوشه\n",
        "\n",
        "if INNER_ZIP_FILE_PATH is None:\n",
        "    print(f\"No inner zip file found in {FIRST_EXTRACT_DIR}. Assuming the first extraction contained the data directly.\")\n",
        "    # اگر زیپ داخلی پیدا نشد، فرض می‌کنیم دیتاست مستقیماً در FIRST_EXTRACT_DIR قرار دارد.\n",
        "    # در این حالت، فقط محتویات را از FIRST_EXTRACT_DIR به FINAL_DATA_EXTRACT_DIR کپی می‌کنیم.\n",
        "    print(f\"Copying contents from {FIRST_EXTRACT_DIR} to {FINAL_DATA_EXTRACT_DIR}...\")\n",
        "    for item in os.listdir(FIRST_EXTRACT_DIR):\n",
        "        s = os.path.join(FIRST_EXTRACT_DIR, item)\n",
        "        d = os.path.join(FINAL_DATA_EXTRACT_DIR, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.copytree(s, d, symlinks=False, ignore_dangling_symlinks=True)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "    print(\"Contents copied successfully.\")\n",
        "else:\n",
        "    print(f\"Found inner zip: {INNER_ZIP_FILE_PATH}. Extracting to {FINAL_DATA_EXTRACT_DIR}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(INNER_ZIP_FILE_PATH, 'r') as inner_zip_ref:\n",
        "            inner_zip_ref.extractall(FINAL_DATA_EXTRACT_DIR)\n",
        "        print(\"Inner dataset extracted successfully.\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: Inner zip file '{INNER_ZIP_FILE_PATH}' is corrupted or not a valid zip. Please check the downloaded dataset.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during inner zip extraction: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- تعیین مسیر ریشه نهایی دیتاست (BASE_DATA_ROOT) ---\n",
        "# اکنون که فایل زیپ داخلی (یا محتویات اولین زیپ) استخراج شده است،\n",
        "# باید پوشه ای را پیدا کنیم که شامل normal, osteopenia, osteoporosis باشد.\n",
        "# این پوشه معمولاً نامی شبیه به \"Knee-X-ray\" یا خود نام دیتاست دارد.\n",
        "BASE_DATA_ROOT = None\n",
        "found_data_dir = False\n",
        "for root, dirs, files in os.walk(FINAL_DATA_EXTRACT_DIR):\n",
        "    # چک می‌کنیم آیا هر سه پوشه کلاس‌های ما در یک دایرکتوری خاص وجود دارند یا خیر.\n",
        "    if all(folder in dirs for folder in ['normal', 'osteopenia', 'osteoporosis']):\n",
        "        BASE_DATA_ROOT = root\n",
        "        found_data_dir = True\n",
        "        break\n",
        "    # همچنین ممکن است پوشه 'Knee-X-ray' یک مرحله بالاتر باشد که شامل این کلاس‌ها باشد.\n",
        "    if 'Knee-X-ray' in dirs and all(folder in os.listdir(os.path.join(root, 'Knee-X-ray')) for folder in ['normal', 'osteopenia', 'osteoporosis']):\n",
        "        BASE_DATA_ROOT = os.path.join(root, 'Knee-X-ray')\n",
        "        found_data_dir = True\n",
        "        break\n",
        "\n",
        "if not found_data_dir:\n",
        "    print(f\"Error: Could not find 'normal', 'osteopenia', 'osteoporosis' directories within {FINAL_DATA_EXTRACT_DIR}.\")\n",
        "    # در این حالت، بهتر است محتویات FINAL_DATA_EXTRACT_DIR را لیست کنیم تا کاربر ببیند مشکل کجاست.\n",
        "    print(\"\\n--- Contents of FINAL_DATA_EXTRACT_DIR ---\")\n",
        "    !ls -R {FINAL_DATA_EXTRACT_DIR}\n",
        "    raise FileNotFoundError(\"Class directories not found. Please verify the dataset structure after extraction.\")\n",
        "\n",
        "\n",
        "print(f\"Final BASE_DATA_ROOT set to: {BASE_DATA_ROOT}\")\n",
        "\n",
        "\n",
        "# --- ۳. تعریف نام کلاس‌ها و جمع‌آوری تمام مسیرهای فایل‌ها و لیبل‌های اصلی ---\n",
        "# نام‌های کلاس‌ها مطابق با ساختار پوشه‌هایی که شما ارسال کردید.\n",
        "CLASS_FOLDERS = ['normal', 'osteopenia', 'osteoporosis']\n",
        "FINAL_NUM_CLASSES = len(CLASS_FOLDERS) # خروجی مدل: 3 کلاس\n",
        "\n",
        "all_filepaths = []\n",
        "all_original_labels = []\n",
        "\n",
        "print(f\"\\nCollecting images from: {BASE_DATA_ROOT} with classes: {CLASS_FOLDERS}\")\n",
        "for class_name in CLASS_FOLDERS:\n",
        "    current_class_path = os.path.join(BASE_DATA_ROOT, class_name)\n",
        "    if not os.path.exists(current_class_path):\n",
        "        print(f\"Warning: Class directory '{current_class_path}' not found. Skipping.\")\n",
        "        continue # اگر پوشه ای نبود، ردش می‌کنیم\n",
        "\n",
        "    for img_name in os.listdir(current_class_path):\n",
        "        if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "            all_filepaths.append(os.path.join(current_class_path, img_name))\n",
        "            all_original_labels.append(class_name)\n",
        "\n",
        "print(f\"Found {len(all_filepaths)} total images for classes: {np.unique(all_original_labels)}.\")\n",
        "\n",
        "# --- ۴. تعریف نگاشت کلاس‌ها (اختیاری، اما برای class_weight مفید است) ---\n",
        "# flow_from_directory به صورت خودکار mapping را بر اساس نام پوشه انجام می‌دهد.\n",
        "# اما برای class_weight باید mapping عددی داشته باشیم.\n",
        "unique_sorted_labels = sorted(np.unique(all_original_labels))\n",
        "CLASS_TO_INT_MAPPING = {label: i for i, label in enumerate(unique_sorted_labels)}\n",
        "INT_TO_CLASS_MAPPING = {i: label for i, label in enumerate(unique_sorted_labels)}\n",
        "\n",
        "print(f\"Class to integer mapping: {CLASS_TO_INT_MAPPING}\")\n",
        "\n",
        "\n",
        "# --- ۵. تنظیم هایپرپارامترها ---\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "TEST_SPLIT_RATIO = 0.2\n",
        "VALIDATION_SPLIT_RATIO = 0.1\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "EPOCHS_CNN = 20\n",
        "EPOCHS_TRANSFER = 15\n",
        "\n",
        "LEARNING_RATE_CNN = 0.001\n",
        "LEARNING_RATE_TRANSFER = 0.0001\n",
        "\n",
        "# --- ۶. آماده‌سازی ساختار موقت برای ImageDataGenerator ---\n",
        "TEMP_SPLIT_DIR = \"/content/temp_dataset_split\"\n",
        "TRAIN_TEMP_DIR = os.path.join(TEMP_SPLIT_DIR, 'train')\n",
        "VAL_TEMP_DIR = os.path.join(TEMP_SPLIT_DIR, 'validation')\n",
        "TEST_TEMP_DIR = os.path.join(TEMP_SPLIT_DIR, 'test')\n",
        "\n",
        "if os.path.exists(TEMP_SPLIT_DIR):\n",
        "    shutil.rmtree(TEMP_SPLIT_DIR)\n",
        "os.makedirs(TRAIN_TEMP_DIR, exist_ok=True)\n",
        "os.makedirs(VAL_TEMP_DIR, exist_ok=True) # این پوشه ها در نهایت توسط ImageDataGenerator ایجاد می‌شوند\n",
        "os.makedirs(TEST_TEMP_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "df_full = pd.DataFrame({'filepath': all_filepaths, 'original_label': all_original_labels})\n",
        "# اضافه کردن ستون برای لیبل‌های عددی\n",
        "df_full['int_label'] = df_full['original_label'].map(CLASS_TO_INT_MAPPING)\n",
        "\n",
        "\n",
        "df_train_val, df_test = train_test_split(\n",
        "    df_full, test_size=TEST_SPLIT_RATIO, random_state=RANDOM_SEED, stratify=df_full['int_label']\n",
        ")\n",
        "\n",
        "def populate_temp_dirs(dataframe, base_target_dir):\n",
        "    # استفاده از tqdm برای نمایش نوار پیشرفت کپی کردن فایل‌ها\n",
        "    for _, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=f\"Copying to {os.path.basename(base_target_dir)}\"):\n",
        "        original_path = row['filepath']\n",
        "        # از لیبل اصلی (نام پوشه) استفاده می‌کنیم، چون flow_from_directory به آن نیاز دارد\n",
        "        class_folder_name = row['original_label']\n",
        "\n",
        "        target_dir = os.path.join(base_target_dir, class_folder_name)\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        target_file = os.path.join(target_dir, os.path.basename(original_path))\n",
        "        if not os.path.exists(target_file):\n",
        "            shutil.copy(original_path, target_file)\n",
        "\n",
        "print(\"\\nPopulating temporary train/test directories...\")\n",
        "populate_temp_dirs(df_train_val, TRAIN_TEMP_DIR)\n",
        "populate_temp_dirs(df_test, TEST_TEMP_DIR)\n",
        "print(\"Temporary directories populated.\")\n",
        "\n",
        "# --- ۷. تعریف تبدیل‌ها (Data Augmentation) و ImageDataGenerator ---\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    validation_split=VALIDATION_SPLIT_RATIO\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_TEMP_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # برای دسته‌بندی چندکلاسه\n",
        "    subset='training',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_TEMP_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # برای دسته‌بندی چندکلاسه\n",
        "    subset='validation',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    TEST_TEMP_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # برای دسته‌بندی چندکلاسه\n",
        "    shuffle=False # برای ارزیابی نهایی، shuffle نباید باشد\n",
        ")\n",
        "\n",
        "# محاسبه class_weights برای مقابله با عدم تعادل کلاس‌ها\n",
        "# از df_train_val برای محاسبه وزن‌ها استفاده می‌کنیم، زیرا این مجموعه واقعی آموزش و اعتبارسنجی است.\n",
        "# از ستون 'int_label' که حاوی لیبل‌های عددی است، استفاده می‌کنیم.\n",
        "unique_classes_for_weight = np.unique(df_train_val['int_label'])\n",
        "calculated_class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=unique_classes_for_weight,\n",
        "    y=df_train_val['int_label']\n",
        ")\n",
        "class_weights_dict = {i: calculated_class_weights[i] for i in range(len(unique_classes_for_weight))}\n",
        "print(f\"Calculated class weights for training: {class_weights_dict}\")\n",
        "\n",
        "# --- ۸. تعریف مدل CNN سفارشی از پایه ---\n",
        "def build_custom_cnn(input_shape, num_classes):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax') # Softmax برای دسته‌بندی چندکلاسه\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "model_cnn = build_custom_cnn(input_shape, FINAL_NUM_CLASSES)\n",
        "model_cnn.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_CNN),\n",
        "                  loss='categorical_crossentropy', # برای دسته‌بندی چندکلاسه\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- مدل CNN سفارشی از پایه ---\")\n",
        "model_cnn.summary()\n",
        "\n",
        "# --- ۹. تعریف مدل یادگیری انتقالی (Transfer Learning - ResNet50) ---\n",
        "def build_transfer_model(input_shape, num_classes):\n",
        "    base_model = keras.applications.ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax') # Softmax برای دسته‌بندی چندکلاسه\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model_transfer = build_transfer_model(input_shape, FINAL_NUM_CLASSES)\n",
        "model_transfer.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_TRANSFER),\n",
        "                       loss='categorical_crossentropy', # برای دسته‌بندی چندکلاسه\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- مدل یادگیری انتقالی (ResNet50) ---\")\n",
        "model_transfer.summary()\n",
        "\n",
        "\n",
        "# --- ۱۰. آموزش و ارزیابی مدل‌ها ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- شروع آموزش Custom CNN ---\")\n",
        "print(\"=\"*50)\n",
        "history_cnn = model_cnn.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS_CNN,\n",
        "    validation_data=validation_generator,\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- شروع آموزش Transfer Learning (ResNet50) ---\")\n",
        "print(\"=\"*50)\n",
        "history_transfer = model_transfer.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS_TRANSFER,\n",
        "    validation_data=validation_generator,\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "# --- ۱۱. ارزیابی نهایی و نمایش نتایج در یک جدول ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- نتایج نهایی ارزیابی روی مجموعه تست ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# این متغیر برای نام کلاس‌ها در گزارش طبقه‌بندی استفاده می‌شود.\n",
        "target_names_for_report = [INT_TO_CLASS_MAPPING[i] for i in sorted(INT_TO_CLASS_MAPPING.keys())]\n",
        "\n",
        "\n",
        "y_pred_cnn_probs = model_cnn.predict(test_generator)\n",
        "y_pred_cnn = np.argmax(y_pred_cnn_probs, axis=1)\n",
        "y_true_test_indices = test_generator.classes # لیبل‌های واقعی تست جنراتور (عددی)\n",
        "\n",
        "cnn_acc = accuracy_score(y_true_test_indices, y_pred_cnn)\n",
        "cnn_prec = precision_score(y_true_test_indices, y_pred_cnn, average='weighted', zero_division=0)\n",
        "cnn_rec = recall_score(y_true_test_indices, y_pred_cnn, average='weighted', zero_division=0)\n",
        "cnn_f1 = f1_score(y_true_test_indices, y_pred_cnn, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"Custom CNN -> Accuracy: {cnn_acc:.4f}, Precision: {cnn_prec:.4f}, Recall: {cnn_rec:.4f}, F1-score: {cnn_f1:.4f}\")\n",
        "print(\"\\nClassification Report for Custom CNN:\")\n",
        "print(classification_report(y_true_test_indices, y_pred_cnn, target_names=target_names_for_report, zero_division=0))\n",
        "print(\"Confusion Matrix for Custom CNN:\")\n",
        "print(confusion_matrix(y_true_test_indices, y_pred_cnn))\n",
        "\n",
        "\n",
        "y_pred_transfer_probs = model_transfer.predict(test_generator)\n",
        "y_pred_transfer = np.argmax(y_pred_transfer_probs, axis=1)\n",
        "\n",
        "transfer_acc = accuracy_score(y_true_test_indices, y_pred_transfer)\n",
        "transfer_prec = precision_score(y_true_test_indices, y_pred_transfer, average='weighted', zero_division=0)\n",
        "transfer_rec = recall_score(y_true_test_indices, y_pred_transfer, average='weighted', zero_division=0)\n",
        "transfer_f1 = f1_score(y_true_test_indices, y_pred_transfer, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"\\nTransfer Learning (ResNet50) -> Accuracy: {transfer_acc:.4f}, Precision: {transfer_prec:.4f}, Recall: {transfer_rec:.4f}, F1-score: {transfer_f1:.4f}\")\n",
        "print(\"\\nClassification Report for Transfer Learning (ResNet50):\")\n",
        "print(classification_report(y_true_test_indices, y_pred_transfer, target_names=target_names_for_report, zero_division=0))\n",
        "print(\"Confusion Matrix for Transfer Learning (ResNet50):\")\n",
        "print(confusion_matrix(y_true_test_indices, y_pred_transfer))\n",
        "\n",
        "\n",
        "data = {\n",
        "    'Model': ['Custom CNN', 'Transfer Learning (ResNet50)'],\n",
        "    'Accuracy': [cnn_acc, transfer_acc],\n",
        "    'Precision': [cnn_prec, transfer_prec],\n",
        "    'Recall': [cnn_rec, transfer_rec],\n",
        "    'F1-score': [cnn_f1, transfer_f1]\n",
        "}\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "print(\"\\n--- جدول نتایج نهایی مقایسه مدل‌ها ---\")\n",
        "print(df_results.round(4).to_markdown(index=False))\n",
        "\n",
        "\n",
        "# --- ۱۲. رسم نمودار Loss و Accuracy در طول آموزش ---\n",
        "def plot_history(history, model_name):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue', marker='o', linestyle='--', markersize=4)\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', marker='x', linestyle='-', markersize=4)\n",
        "    plt.title(f'{model_name} - Accuracy over Epochs', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', color='green', marker='o', linestyle='--', markersize=4)\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', color='purple', marker='x', linestyle='-', markersize=4)\n",
        "    plt.title(f'{model_name} - Loss over Epochs', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- رسم نمودارهای آموزش ---\")\n",
        "plot_history(history_cnn, \"Custom CNN Model\")\n",
        "plot_history(history_transfer, \"Transfer Learning (ResNet50) Model\")\n",
        "\n",
        "# --- ۱۳. پاکسازی پوشه‌های موقت ---\n",
        "print(\"\\n--- Cleaning up temporary directories and downloaded files ---\")\n",
        "if os.path.exists(FIRST_EXTRACT_DIR):\n",
        "    shutil.rmtree(FIRST_EXTRACT_DIR)\n",
        "    print(f\"Removed first extraction directory: '{FIRST_EXTRACT_DIR}'.\")\n",
        "if os.path.exists(FINAL_DATA_EXTRACT_DIR) and FINAL_DATA_EXTRACT_DIR != FIRST_EXTRACT_DIR:\n",
        "    shutil.rmtree(FINAL_DATA_EXTRACT_DIR)\n",
        "    print(f\"Removed final dataset extraction directory: '{FINAL_DATA_EXTRACT_DIR}'.\")\n",
        "if os.path.exists(TEMP_SPLIT_DIR):\n",
        "    shutil.rmtree(TEMP_SPLIT_DIR)\n",
        "    print(f\"Removed temporary data split directory: '{TEMP_SPLIT_DIR}'.\")\n",
        "if os.path.exists(OUTER_ZIP_FILE_NAME):\n",
        "    os.remove(OUTER_ZIP_FILE_NAME)\n",
        "    print(f\"Removed downloaded outer zip file: '{OUTER_ZIP_FILE_NAME}'.\")\n",
        "if INNER_ZIP_FILE_PATH and os.path.exists(INNER_ZIP_FILE_PATH):\n",
        "    # این خط فقط اگر زیپ داخلی در یک مسیر جداگانه باقی مانده باشد آن را حذف می‌کند\n",
        "    # (نه اگر بخشی از محتوای FIRST_EXTRACT_DIR باشد که خودش حذف می‌شود)\n",
        "    if not INNER_ZIP_FILE_PATH.startswith(FIRST_EXTRACT_DIR) or not os.path.exists(FIRST_EXTRACT_DIR):\n",
        "        os.remove(INNER_ZIP_FILE_PATH)\n",
        "        print(f\"Removed inner zip file: '{INNER_ZIP_FILE_PATH}'.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- پایان اجرای برنامه ---\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shutil # برای مدیریت فایل‌ها (ساخت و حذف پوشه‌ها)\n",
        "import zipfile # برای کار با فایل‌های زیپ\n",
        "from tqdm import tqdm # برای نمایش نوار پیشرفت در هنگام کپی فایل‌ها\n",
        "\n",
        "# --- ۰. نصب و به‌روزرسانی کتابخانه‌ها (مخصوص Colab) ---\n",
        "print(\"Installing and upgrading necessary libraries...\")\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install scikit-learn pandas matplotlib tqdm\n",
        "print(\"Libraries installed/upgraded successfully.\")\n",
        "\n",
        "# --- ۱. تنظیمات اولیه و بررسی دسترسی به GPU ---\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(f\"Using {len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found, using CPU.\")\n",
        "\n",
        "# --- ۲. دانلود و آماده‌سازی دیتاست در Colab (با مدیریت Double-Zipping) ---\n",
        "print(\"\\n--- Downloading and preparing dataset (handling nested zips) ---\")\n",
        "DATASET_URL = \"https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/2617b452-9878-477e-b0e7-4f7bfe7ea873\"\n",
        "OUTER_ZIP_FILE_NAME = \"Knee-X-ray_outer.zip\"\n",
        "FIRST_EXTRACT_DIR = \"/content/first_extracted_zip_contents\" # مسیری برای اولین استخراج\n",
        "\n",
        "# دانلود فایل زیپ بیرونی\n",
        "print(f\"Downloading dataset from {DATASET_URL}...\")\n",
        "# اگر لینک دانلود مجددا خراب شد، ممکن است نیاز به لینک جدید باشد یا آپلود دستی.\n",
        "!wget -O {OUTER_ZIP_FILE_NAME} \"{DATASET_URL}\"\n",
        "print(f\"Downloaded {OUTER_ZIP_FILE_NAME} successfully.\")\n",
        "\n",
        "# ایجاد دایرکتوری برای اولین استخراج\n",
        "os.makedirs(FIRST_EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "# استخراج فایل زیپ بیرونی\n",
        "print(f\"Extracting {OUTER_ZIP_FILE_NAME} to {FIRST_EXTRACT_DIR}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(OUTER_ZIP_FILE_NAME, 'r') as zip_ref:\n",
        "        zip_ref.extractall(FIRST_EXTRACT_DIR)\n",
        "    print(\"First level extraction complete.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: {OUTER_ZIP_FILE_NAME} is not a valid zip file or is corrupted. Attempting to proceed assuming it's an empty or problematic zip, check manually.\")\n",
        "    # در این حالت، ممکن است zipfile.BadZipFile رخ دهد اما wget فایل را دانلود کرده باشد.\n",
        "    # باید مطمئن شویم که یا یک فایل زیپ داخلی هست یا خطا جدی‌تر است.\n",
        "    # فعلا اجازه می‌دهیم کد ادامه یابد تا شاید زیپ داخلی را پیدا کند.\n",
        "\n",
        "\n",
        "# --- جستجو و استخراج فایل زیپ داخلی ---\n",
        "INNER_ZIP_FILE_PATH = None\n",
        "# جستجو در پوشه استخراج شده اول برای یافتن فایل زیپ داخلی\n",
        "for root, dirs, files in os.walk(FIRST_EXTRACT_DIR):\n",
        "    for file in files:\n",
        "        if file.lower().endswith('.zip'):\n",
        "            INNER_ZIP_FILE_PATH = os.path.join(root, file)\n",
        "            break\n",
        "    if INNER_ZIP_FILE_PATH:\n",
        "        break\n",
        "\n",
        "FINAL_DATA_EXTRACT_DIR = \"/content/final_dataset_extracted\" # مسیر نهایی برای استخراج دیتاست واقعی\n",
        "os.makedirs(FINAL_DATA_EXTRACT_DIR, exist_ok=True) # اطمینان از وجود این پوشه\n",
        "\n",
        "if INNER_ZIP_FILE_PATH is None:\n",
        "    print(f\"No inner zip file found in {FIRST_EXTRACT_DIR}. Assuming the first extraction contained the data directly.\")\n",
        "    # اگر زیپ داخلی پیدا نشد، فرض می‌کنیم دیتاست مستقیماً در FIRST_EXTRACT_DIR قرار دارد.\n",
        "    # در این حالت، فقط محتویات را از FIRST_EXTRACT_DIR به FINAL_DATA_EXTRACT_DIR کپی می‌کنیم.\n",
        "    print(f\"Copying contents from {FIRST_EXTRACT_DIR} to {FINAL_DATA_EXTRACT_DIR}...\")\n",
        "    for item in os.listdir(FIRST_EXTRACT_DIR):\n",
        "        s = os.path.join(FIRST_EXTRACT_DIR, item)\n",
        "        d = os.path.join(FINAL_DATA_EXTRACT_DIR, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.copytree(s, d, symlinks=False, ignore_dangling_symlinks=True)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "    print(\"Contents copied successfully.\")\n",
        "else:\n",
        "    print(f\"Found inner zip: {INNER_ZIP_FILE_PATH}. Extracting to {FINAL_DATA_EXTRACT_DIR}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(INNER_ZIP_FILE_PATH, 'r') as inner_zip_ref:\n",
        "            inner_zip_ref.extractall(FINAL_DATA_EXTRACT_DIR)\n",
        "        print(\"Inner dataset extracted successfully.\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: Inner zip file '{INNER_ZIP_FILE_PATH}' is corrupted or not a valid zip. Please check the downloaded dataset.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during inner zip extraction: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- تعیین مسیر ریشه نهایی دیتاست (BASE_DATA_ROOT) ---\n",
        "# اکنون که فایل زیپ داخلی (یا محتویات اولین زیپ) استخراج شده است،\n",
        "# باید پوشه ای را پیدا کنیم که شامل normal, osteopenia, osteoporosis باشد.\n",
        "# این پوشه معمولاً نامی شبیه به \"Knee-X-ray\" یا خود نام دیتاست دارد.\n",
        "BASE_DATA_ROOT = None\n",
        "found_data_dir = False\n",
        "for root, dirs, files in os.walk(FINAL_DATA_EXTRACT_DIR):\n",
        "    # چک می‌کنیم آیا هر سه پوشه کلاس‌های ما در یک دایرکتوری خاص وجود دارند یا خیر.\n",
        "    if all(folder in dirs for folder in ['normal', 'osteopenia', 'osteoporosis']):\n",
        "        BASE_DATA_ROOT = root\n",
        "        found_data_dir = True\n",
        "        break\n",
        "    # همچنین ممکن است پوشه 'Knee-X-ray' یک مرحله بالاتر باشد که شامل این کلاس‌ها باشد.\n",
        "    if 'Knee-X-ray' in dirs and all(folder in os.listdir(os.path.join(root, 'Knee-X-ray')) for folder in ['normal', 'osteopenia', 'osteoporosis']):\n",
        "        BASE_DATA_ROOT = os.path.join(root, 'Knee-X-ray')\n",
        "        found_data_dir = True\n",
        "        break\n",
        "\n",
        "if not found_data_dir:\n",
        "    print(f\"Error: Could not find 'normal', 'osteopenia', 'osteoporosis' directories within {FINAL_DATA_EXTRACT_DIR}.\")\n",
        "    # در این حالت، بهتر است محتویات FINAL_DATA_EXTRACT_DIR را لیست کنیم تا کاربر ببیند مشکل کجاست.\n",
        "    print(\"\\n--- Contents of FINAL_DATA_EXTRACT_DIR ---\")\n",
        "    !ls -R {FINAL_DATA_EXTRACT_DIR}\n",
        "    raise FileNotFoundError(\"Class directories not found. Please verify the dataset structure after extraction.\")\n",
        "\n",
        "\n",
        "print(f\"Final BASE_DATA_ROOT set to: {BASE_DATA_ROOT}\")\n",
        "\n",
        "\n",
        "# --- ۳. تعریف نام کلاس‌ها و جمع‌آوری تمام مسیرهای فایل‌ها و لیبل‌های اصلی ---\n",
        "# نام‌های کلاس‌ها مطابق با ساختار پوشه‌هایی که شما ارسال کردید.\n",
        "CLASS_FOLDERS = ['normal', 'osteopenia', 'osteoporosis']\n",
        "FINAL_NUM_CLASSES = len(CLASS_FOLDERS) # خروجی مدل: 3 کلاس\n",
        "\n",
        "all_filepaths = []\n",
        "all_original_labels = []\n",
        "\n",
        "print(f\"\\nCollecting images from: {BASE_DATA_ROOT} with classes: {CLASS_FOLDERS}\")\n",
        "for class_name in CLASS_FOLDERS:\n",
        "    current_class_path = os.path.join(BASE_DATA_ROOT, class_name)\n",
        "    if not os.path.exists(current_class_path):\n",
        "        print(f\"Warning: Class directory '{current_class_path}' not found. Skipping.\")\n",
        "        continue # اگر پوشه ای نبود، ردش می‌کنیم\n",
        "\n",
        "    for img_name in os.listdir(current_class_path):\n",
        "        if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "            all_filepaths.append(os.path.join(current_class_path, img_name))\n",
        "            all_original_labels.append(class_name)\n",
        "\n",
        "print(f\"Found {len(all_filepaths)} total images for classes: {np.unique(all_original_labels)}.\")\n",
        "\n",
        "# --- ۴. تعریف نگاشت کلاس‌ها (اختیاری، اما برای class_weight مفید است) ---\n",
        "# flow_from_directory به صورت خودکار mapping را بر اساس نام پوشه انجام می‌دهد.\n",
        "# اما برای class_weight باید mapping عددی داشته باشیم.\n",
        "unique_sorted_labels = sorted(np.unique(all_original_labels))\n",
        "CLASS_TO_INT_MAPPING = {label: i for i, label in enumerate(unique_sorted_labels)}\n",
        "INT_TO_CLASS_MAPPING = {i: label for i, label in enumerate(unique_sorted_labels)}\n",
        "\n",
        "print(f\"Class to integer mapping: {CLASS_TO_INT_MAPPING}\")\n",
        "\n",
        "\n",
        "# --- ۵. تنظیم هایپرپارامترها ---\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "TEST_SPLIT_RATIO = 0.2\n",
        "VALIDATION_SPLIT_RATIO = 0.1\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "EPOCHS_CNN = 20\n",
        "EPOCHS_TRANSFER = 15\n",
        "\n",
        "LEARNING_RATE_CNN = 0.001\n",
        "LEARNING_RATE_TRANSFER = 0.0001\n",
        "\n",
        "# --- ۶. آماده‌سازی ساختار موقت برای ImageDataGenerator ---\n",
        "TEMP_SPLIT_DIR = \"/content/temp_dataset_split\"\n",
        "TRAIN_TEMP_DIR = os.path.join(TEMP_SPLIT_DIR, 'train')\n",
        "VAL_TEMP_DIR = os.path.join(TEMP_SPLIT_DIR, 'validation')\n",
        "TEST_TEMP_DIR = os.path.join(TEMP_SPLIT_DIR, 'test')\n",
        "\n",
        "if os.path.exists(TEMP_SPLIT_DIR):\n",
        "    shutil.rmtree(TEMP_SPLIT_DIR)\n",
        "os.makedirs(TRAIN_TEMP_DIR, exist_ok=True)\n",
        "os.makedirs(VAL_TEMP_DIR, exist_ok=True) # این پوشه ها در نهایت توسط ImageDataGenerator ایجاد می‌شوند\n",
        "os.makedirs(TEST_TEMP_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "df_full = pd.DataFrame({'filepath': all_filepaths, 'original_label': all_original_labels})\n",
        "# اضافه کردن ستون برای لیبل‌های عددی\n",
        "df_full['int_label'] = df_full['original_label'].map(CLASS_TO_INT_MAPPING)\n",
        "\n",
        "\n",
        "df_train_val, df_test = train_test_split(\n",
        "    df_full, test_size=TEST_SPLIT_RATIO, random_state=RANDOM_SEED, stratify=df_full['int_label']\n",
        ")\n",
        "\n",
        "def populate_temp_dirs(dataframe, base_target_dir):\n",
        "    # استفاده از tqdm برای نمایش نوار پیشرفت کپی کردن فایل‌ها\n",
        "    for _, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=f\"Copying to {os.path.basename(base_target_dir)}\"):\n",
        "        original_path = row['filepath']\n",
        "        # از لیبل اصلی (نام پوشه) استفاده می‌کنیم، چون flow_from_directory به آن نیاز دارد\n",
        "        class_folder_name = row['original_label']\n",
        "\n",
        "        target_dir = os.path.join(base_target_dir, class_folder_name)\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        target_file = os.path.join(target_dir, os.path.basename(original_path))\n",
        "        if not os.path.exists(target_file):\n",
        "            shutil.copy(original_path, target_file)\n",
        "\n",
        "print(\"\\nPopulating temporary train/test directories...\")\n",
        "populate_temp_dirs(df_train_val, TRAIN_TEMP_DIR)\n",
        "populate_temp_dirs(df_test, TEST_TEMP_DIR)\n",
        "print(\"Temporary directories populated.\")\n",
        "\n",
        "# --- ۷. تعریف تبدیل‌ها (Data Augmentation) و ImageDataGenerator ---\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    validation_split=VALIDATION_SPLIT_RATIO\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_TEMP_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # برای دسته‌بندی چندکلاسه\n",
        "    subset='training',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_TEMP_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # برای دسته‌بندی چندکلاسه\n",
        "    subset='validation',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    TEST_TEMP_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # برای دسته‌بندی چندکلاسه\n",
        "    shuffle=False # برای ارزیابی نهایی، shuffle نباید باشد\n",
        ")\n",
        "\n",
        "# محاسبه class_weights برای مقابله با عدم تعادل کلاس‌ها\n",
        "# از df_train_val برای محاسبه وزن‌ها استفاده می‌کنیم، زیرا این مجموعه واقعی آموزش و اعتبارسنجی است.\n",
        "# از ستون 'int_label' که حاوی لیبل‌های عددی است، استفاده می‌کنیم.\n",
        "unique_classes_for_weight = np.unique(df_train_val['int_label'])\n",
        "calculated_class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=unique_classes_for_weight,\n",
        "    y=df_train_val['int_label']\n",
        ")\n",
        "class_weights_dict = {i: calculated_class_weights[i] for i in range(len(unique_classes_for_weight))}\n",
        "print(f\"Calculated class weights for training: {class_weights_dict}\")\n",
        "\n",
        "# --- ۸. تعریف مدل CNN سفارشی از پایه ---\n",
        "def build_custom_cnn(input_shape, num_classes):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax') # Softmax برای دسته‌بندی چندکلاسه\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "model_cnn = build_custom_cnn(input_shape, FINAL_NUM_CLASSES)\n",
        "model_cnn.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_CNN),\n",
        "                  loss='categorical_crossentropy', # برای دسته‌بندی چندکلاسه\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- مدل CNN سفارشی از پایه ---\")\n",
        "model_cnn.summary()\n",
        "\n",
        "# --- ۹. تعریف مدل یادگیری انتقالی (Transfer Learning - ResNet50) ---\n",
        "def build_transfer_model(input_shape, num_classes):\n",
        "    base_model = keras.applications.ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax') # Softmax برای دسته‌بندی چندکلاسه\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model_transfer = build_transfer_model(input_shape, FINAL_NUM_CLASSES)\n",
        "model_transfer.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_TRANSFER),\n",
        "                       loss='categorical_crossentropy', # برای دسته‌بندی چندکلاسه\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- مدل یادگیری انتقالی (ResNet50) ---\")\n",
        "model_transfer.summary()\n",
        "\n",
        "\n",
        "# --- ۱۰. آموزش و ارزیابی مدل‌ها ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- شروع آموزش Custom CNN ---\")\n",
        "print(\"=\"*50)\n",
        "history_cnn = model_cnn.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS_CNN,\n",
        "    validation_data=validation_generator,\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- شروع آموزش Transfer Learning (ResNet50) ---\")\n",
        "print(\"=\"*50)\n",
        "history_transfer = model_transfer.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS_TRANSFER,\n",
        "    validation_data=validation_generator,\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "# --- ۱۱. ارزیابی نهایی و نمایش نتایج در یک جدول ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- نتایج نهایی ارزیابی روی مجموعه تست ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# این متغیر برای نام کلاس‌ها در گزارش طبقه‌بندی استفاده می‌شود.\n",
        "target_names_for_report = [INT_TO_CLASS_MAPPING[i] for i in sorted(INT_TO_CLASS_MAPPING.keys())]\n",
        "\n",
        "\n",
        "y_pred_cnn_probs = model_cnn.predict(test_generator)\n",
        "y_pred_cnn = np.argmax(y_pred_cnn_probs, axis=1)\n",
        "y_true_test_indices = test_generator.classes # لیبل‌های واقعی تست جنراتور (عددی)\n",
        "\n",
        "cnn_acc = accuracy_score(y_true_test_indices, y_pred_cnn)\n",
        "cnn_prec = precision_score(y_true_test_indices, y_pred_cnn, average='weighted', zero_division=0)\n",
        "cnn_rec = recall_score(y_true_test_indices, y_pred_cnn, average='weighted', zero_division=0)\n",
        "cnn_f1 = f1_score(y_true_test_indices, y_pred_cnn, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"Custom CNN -> Accuracy: {cnn_acc:.4f}, Precision: {cnn_prec:.4f}, Recall: {cnn_rec:.4f}, F1-score: {cnn_f1:.4f}\")\n",
        "print(\"\\nClassification Report for Custom CNN:\")\n",
        "print(classification_report(y_true_test_indices, y_pred_cnn, target_names=target_names_for_report, zero_division=0))\n",
        "print(\"Confusion Matrix for Custom CNN:\")\n",
        "print(confusion_matrix(y_true_test_indices, y_pred_cnn))\n",
        "\n",
        "\n",
        "y_pred_transfer_probs = model_transfer.predict(test_generator)\n",
        "y_pred_transfer = np.argmax(y_pred_transfer_probs, axis=1)\n",
        "\n",
        "transfer_acc = accuracy_score(y_true_test_indices, y_pred_transfer)\n",
        "transfer_prec = precision_score(y_true_test_indices, y_pred_transfer, average='weighted', zero_division=0)\n",
        "transfer_rec = recall_score(y_true_test_indices, y_pred_transfer, average='weighted', zero_division=0)\n",
        "transfer_f1 = f1_score(y_true_test_indices, y_pred_transfer, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"\\nTransfer Learning (ResNet50) -> Accuracy: {transfer_acc:.4f}, Precision: {transfer_prec:.4f}, Recall: {transfer_rec:.4f}, F1-score: {transfer_f1:.4f}\")\n",
        "print(\"\\nClassification Report for Transfer Learning (ResNet50):\")\n",
        "print(classification_report(y_true_test_indices, y_pred_transfer, target_names=target_names_for_report, zero_division=0))\n",
        "print(\"Confusion Matrix for Transfer Learning (ResNet50):\")\n",
        "print(confusion_matrix(y_true_test_indices, y_pred_transfer))\n",
        "\n",
        "\n",
        "data = {\n",
        "    'Model': ['Custom CNN', 'Transfer Learning (ResNet50)'],\n",
        "    'Accuracy': [cnn_acc, transfer_acc],\n",
        "    'Precision': [cnn_prec, transfer_prec],\n",
        "    'Recall': [cnn_rec, transfer_rec],\n",
        "    'F1-score': [cnn_f1, transfer_f1]\n",
        "}\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "print(\"\\n--- جدول نتایج نهایی مقایسه مدل‌ها ---\")\n",
        "print(df_results.round(4).to_markdown(index=False))\n",
        "\n",
        "\n",
        "# --- ۱۲. رسم نمودار Loss و Accuracy در طول آموزش ---\n",
        "def plot_history(history, model_name):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue', marker='o', linestyle='--', markersize=4)\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', marker='x', linestyle='-', markersize=4)\n",
        "    plt.title(f'{model_name} - Accuracy over Epochs', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', color='green', marker='o', linestyle='--', markersize=4)\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', color='purple', marker='x', linestyle='-', markersize=4)\n",
        "    plt.title(f'{model_name} - Loss over Epochs', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- رسم نمودارهای آموزش ---\")\n",
        "plot_history(history_cnn, \"Custom CNN Model\")\n",
        "plot_history(history_transfer, \"Transfer Learning (ResNet50) Model\")\n",
        "\n",
        "# --- ۱۳. پاکسازی پوشه‌های موقت ---\n",
        "print(\"\\n--- Cleaning up temporary directories and downloaded files ---\")\n",
        "if os.path.exists(FIRST_EXTRACT_DIR):\n",
        "    shutil.rmtree(FIRST_EXTRACT_DIR)\n",
        "    print(f\"Removed first extraction directory: '{FIRST_EXTRACT_DIR}'.\")\n",
        "if os.path.exists(FINAL_DATA_EXTRACT_DIR) and FINAL_DATA_EXTRACT_DIR != FIRST_EXTRACT_DIR:\n",
        "    shutil.rmtree(FINAL_DATA_EXTRACT_DIR)\n",
        "    print(f\"Removed final dataset extraction directory: '{FINAL_DATA_EXTRACT_DIR}'.\")\n",
        "if os.path.exists(TEMP_SPLIT_DIR):\n",
        "    shutil.rmtree(TEMP_SPLIT_DIR)\n",
        "    print(f\"Removed temporary data split directory: '{TEMP_SPLIT_DIR}'.\")\n",
        "if os.path.exists(OUTER_ZIP_FILE_NAME):\n",
        "    os.remove(OUTER_ZIP_FILE_NAME)\n",
        "    print(f\"Removed downloaded outer zip file: '{OUTER_ZIP_FILE_NAME}'.\")\n",
        "if INNER_ZIP_FILE_PATH and os.path.exists(INNER_ZIP_FILE_PATH):\n",
        "    # این خط فقط اگر زیپ داخلی در یک مسیر جداگانه باقی مانده باشد آن را حذف می‌کند\n",
        "    # (نه اگر بخشی از محتوای FIRST_EXTRACT_DIR باشد که خودش حذف می‌شود)\n",
        "    if not INNER_ZIP_FILE_PATH.startswith(FIRST_EXTRACT_DIR) or not os.path.exists(FIRST_EXTRACT_DIR):\n",
        "        os.remove(INNER_ZIP_FILE_PATH)\n",
        "        print(f\"Removed inner zip file: '{INNER_ZIP_FILE_PATH}'.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- پایان اجرای برنامه ---\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sjbiHKhljDaf",
        "outputId": "1ccd7cea-8942-4325-a723-75d65bdf57be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing and upgrading necessary libraries...\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Libraries installed/upgraded successfully.\n",
            "No GPU found, using CPU.\n",
            "\n",
            "--- Downloading and preparing dataset (handling nested zips) ---\n",
            "Downloading dataset from https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/2617b452-9878-477e-b0e7-4f7bfe7ea873...\n",
            "--2025-06-07 08:40:49--  https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/2617b452-9878-477e-b0e7-4f7bfe7ea873\n",
            "Resolving prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com)... 3.5.65.90, 52.218.62.32, 52.218.98.96, ...\n",
            "Connecting to prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com (prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com)|3.5.65.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 190531319 (182M) [application/x-zip-compressed]\n",
            "Saving to: ‘Knee-X-ray_outer.zip’\n",
            "\n",
            "Knee-X-ray_outer.zi 100%[===================>] 181.70M  13.5MB/s    in 16s     \n",
            "\n",
            "2025-06-07 08:41:06 (11.5 MB/s) - ‘Knee-X-ray_outer.zip’ saved [190531319/190531319]\n",
            "\n",
            "Downloaded Knee-X-ray_outer.zip successfully.\n",
            "Extracting Knee-X-ray_outer.zip to /content/first_extracted_zip_contents...\n",
            "First level extraction complete.\n",
            "No inner zip file found in /content/first_extracted_zip_contents. Assuming the first extraction contained the data directly.\n",
            "Copying contents from /content/first_extracted_zip_contents to /content/final_dataset_extracted...\n",
            "Contents copied successfully.\n",
            "Final BASE_DATA_ROOT set to: /content/final_dataset_extracted/Osteoporosis Knee X-ray\n",
            "\n",
            "Collecting images from: /content/final_dataset_extracted/Osteoporosis Knee X-ray with classes: ['normal', 'osteopenia', 'osteoporosis']\n",
            "Found 239 total images for classes: ['normal' 'osteopenia' 'osteoporosis'].\n",
            "Class to integer mapping: {np.str_('normal'): 0, np.str_('osteopenia'): 1, np.str_('osteoporosis'): 2}\n",
            "\n",
            "Populating temporary train/test directories...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying to train: 100%|██████████| 191/191 [00:00<00:00, 272.24it/s]\n",
            "Copying to test: 100%|██████████| 48/48 [00:00<00:00, 193.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporary directories populated.\n",
            "Found 174 images belonging to 3 classes.\n",
            "Found 17 images belonging to 3 classes.\n",
            "Found 48 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated class weights for training: {0: np.float64(2.1954022988505746), 1: np.float64(0.5176151761517616), 2: np.float64(1.6324786324786325)}\n",
            "\n",
            "--- مدل CNN سفارشی از پایه ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100352\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m25,690,368\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,690,368</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,013,987\u001b[0m (99.24 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,013,987</span> (99.24 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,012,323\u001b[0m (99.23 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,012,323</span> (99.23 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,664\u001b[0m (6.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> (6.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "\n",
            "--- مدل یادگیری انتقالی (ResNet50) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,147,075\u001b[0m (92.11 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,147,075</span> (92.11 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m558,595\u001b[0m (2.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">558,595</span> (2.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,588,480\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,588,480</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "--- شروع آموزش Custom CNN ---\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 14s/step - accuracy: 0.3074 - loss: 2.5255 - val_accuracy: 0.1765 - val_loss: 2.2517\n",
            "Epoch 2/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 13s/step - accuracy: 0.3703 - loss: 1.6921 - val_accuracy: 0.1765 - val_loss: 6.8708\n",
            "Epoch 3/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 13s/step - accuracy: 0.4050 - loss: 1.4989 - val_accuracy: 0.1765 - val_loss: 8.8403\n",
            "Epoch 4/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 15s/step - accuracy: 0.3476 - loss: 2.1862 - val_accuracy: 0.1765 - val_loss: 9.6282\n",
            "Epoch 5/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 15s/step - accuracy: 0.3596 - loss: 1.8515 - val_accuracy: 0.1765 - val_loss: 9.0648\n",
            "Epoch 6/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 15s/step - accuracy: 0.2856 - loss: 2.0878 - val_accuracy: 0.1765 - val_loss: 8.5725\n",
            "Epoch 7/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 14s/step - accuracy: 0.3637 - loss: 1.6654 - val_accuracy: 0.1765 - val_loss: 7.4285\n",
            "Epoch 8/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 14s/step - accuracy: 0.3684 - loss: 1.8110 - val_accuracy: 0.1765 - val_loss: 6.4790\n",
            "Epoch 9/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 14s/step - accuracy: 0.4354 - loss: 1.7091 - val_accuracy: 0.1765 - val_loss: 5.6264\n",
            "Epoch 10/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 13s/step - accuracy: 0.3959 - loss: 1.6115 - val_accuracy: 0.1176 - val_loss: 5.5162\n",
            "Epoch 11/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 13s/step - accuracy: 0.3283 - loss: 1.6187 - val_accuracy: 0.1176 - val_loss: 5.1753\n",
            "Epoch 12/20\n",
            "\u001b[1m5/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 17s/step - accuracy: 0.3556 - loss: 1.7640"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shutil # برای مدیریت فایل‌ها (ساخت و حذف پوشه‌ها)\n",
        "import zipfile # برای کار با فایل‌های زیپ\n",
        "from tqdm import tqdm # برای نمایش نوار پیشرفت در هنگام کپی فایل‌ها\n",
        "\n",
        "# --- ۰. نصب و به‌روزرسانی کتابخانه‌ها (مخصوص Colab) ---\n",
        "print(\"Installing and upgrading necessary libraries...\")\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install scikit-learn pandas matplotlib tqdm\n",
        "print(\"Libraries installed/upgraded successfully.\")\n",
        "\n",
        "# --- ۱. تنظیمات اولیه و بررسی دسترسی به GPU ---\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "        print(f\"Using {len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found, using CPU.\")\n",
        "\n",
        "# --- ۲. دانلود و آماده‌سازی دیتاست در Colab (با مدیریت Double-Zipping) ---\n",
        "print(\"\\n--- Downloading and preparing dataset (handling nested zips) ---\")\n",
        "DATASET_URL = \"https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/2617b452-9878-477e-b0e7-4f7bfe7ea873\"\n",
        "OUTER_ZIP_FILE_NAME = \"Knee-X-ray_outer.zip\"\n",
        "FIRST_EXTRACT_DIR = \"/content/first_extracted_zip_contents\" # مسیری برای اولین استخراج\n",
        "\n",
        "# دانلود فایل زیپ بیرونی\n",
        "print(f\"Downloading dataset from {DATASET_URL}...\")\n",
        "# اگر لینک دانلود مجددا خراب شد، ممکن است نیاز به لینک جدید باشد یا آپلود دستی.\n",
        "!wget -O {OUTER_ZIP_FILE_NAME} \"{DATASET_URL}\"\n",
        "print(f\"Downloaded {OUTER_ZIP_FILE_NAME} successfully.\")\n",
        "\n",
        "# ایجاد دایرکتوری برای اولین استخراج\n",
        "os.makedirs(FIRST_EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "# استخراج فایل زیپ بیرونی\n",
        "print(f\"Extracting {OUTER_ZIP_FILE_NAME} to {FIRST_EXTRACT_DIR}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(OUTER_ZIP_FILE_NAME, 'r') as zip_ref:\n",
        "        zip_ref.extractall(FIRST_EXTRACT_DIR)\n",
        "    print(\"First level extraction complete.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Error: {OUTER_ZIP_FILE_NAME} is not a valid zip file or is corrupted. Attempting to proceed assuming it's an empty or problematic zip, check manually.\")\n",
        "    # در این حالت، ممکن است zipfile.BadZipFile رخ دهد اما wget فایل را دانلود کرده باشد.\n",
        "    # باید مطمئن شویم که یا یک فایل زیپ داخلی هست یا خطا جدی‌تر است.\n",
        "    # فعلا اجازه می‌دهیم کد ادامه یابد تا شاید زیپ داخلی را پیدا کند.\n",
        "\n",
        "\n",
        "# --- جستجو و استخراج فایل زیپ داخلی ---\n",
        "INNER_ZIP_FILE_PATH = None\n",
        "# جستجو در پوشه استخراج شده اول برای یافتن فایل زیپ داخلی\n",
        "for root, dirs, files in os.walk(FIRST_EXTRACT_DIR):\n",
        "    for file in files:\n",
        "        if file.lower().endswith('.zip'):\n",
        "            INNER_ZIP_FILE_PATH = os.path.join(root, file)\n",
        "            break\n",
        "    if INNER_ZIP_FILE_PATH:\n",
        "        break\n",
        "\n",
        "FINAL_DATA_EXTRACT_DIR = \"/content/final_dataset_extracted\" # مسیر نهایی برای استخراج دیتاست واقعی\n",
        "os.makedirs(FINAL_DATA_EXTRACT_DIR, exist_ok=True) # اطمینان از وجود این پوشه\n",
        "\n",
        "if INNER_ZIP_FILE_PATH is None:\n",
        "    print(f\"No inner zip file found in {FIRST_EXTRACT_DIR}. Assuming the first extraction contained the data directly.\")\n",
        "    # اگر زیپ داخلی پیدا نشد، فرض می‌کنیم دیتاست مستقیماً در FIRST_EXTRACT_DIR قرار دارد.\n",
        "    # در این حالت، فقط محتویات را از FIRST_EXTRACT_DIR به FINAL_DATA_EXTRACT_DIR کپی می‌کنیم.\n",
        "    print(f\"Copying contents from {FIRST_EXTRACT_DIR} to {FINAL_DATA_EXTRACT_DIR}...\")\n",
        "    for item in os.listdir(FIRST_EXTRACT_DIR):\n",
        "        s = os.path.join(FIRST_EXTRACT_DIR, item)\n",
        "        d = os.path.join(FINAL_DATA_EXTRACT_DIR, item)\n",
        "        if os.path.isdir(s):\n",
        "            shutil.copytree(s, d, symlinks=False, ignore_dangling_symlinks=True)\n",
        "        else:\n",
        "            shutil.copy2(s, d)\n",
        "    print(\"Contents copied successfully.\")\n",
        "else:\n",
        "    print(f\"Found inner zip: {INNER_ZIP_FILE_PATH}. Extracting to {FINAL_DATA_EXTRACT_DIR}...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(INNER_ZIP_FILE_PATH, 'r') as inner_zip_ref:\n",
        "            inner_zip_ref.extractall(FINAL_DATA_EXTRACT_DIR)\n",
        "        print(\"Inner dataset extracted successfully.\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: Inner zip file '{INNER_ZIP_FILE_PATH}' is corrupted or not a valid zip. Please check the downloaded dataset.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during inner zip extraction: {e}\")\n",
        "        raise\n",
        "\n",
        "# --- تعیین مسیر ریشه نهایی دیتاست (BASE_DATA_ROOT) ---\n",
        "# اکنون که فایل زیپ داخلی (یا محتویات اولین زیپ) استخراج شده است،\n",
        "# باید پوشه ای را پیدا کنیم که شامل normal, osteopenia, osteoporosis باشد.\n",
        "# این پوشه معمولاً نامی شبیه به \"Knee-X-ray\" یا خود نام دیتاست دارد.\n",
        "BASE_DATA_ROOT = None\n",
        "found_data_dir = False\n",
        "for root, dirs, files in os.walk(FINAL_DATA_EXTRACT_DIR):\n",
        "    # چک می‌کنیم آیا هر سه پوشه کلاس‌های ما در یک دایرکتوری خاص وجود دارند یا خیر.\n",
        "    if all(folder in dirs for folder in ['normal', 'osteopenia', 'osteoporosis']):\n",
        "        BASE_DATA_ROOT = root\n",
        "        found_data_dir = True\n",
        "        break\n",
        "    # همچنین ممکن است پوشه 'Knee-X-ray' یک مرحله بالاتر باشد که شامل این کلاس‌ها باشد.\n",
        "    if 'Knee-X-ray' in dirs and all(folder in os.listdir(os.path.join(root, 'Knee-X-ray')) for folder in ['normal', 'osteopenia', 'osteoporosis']):\n",
        "        BASE_DATA_ROOT = os.path.join(root, 'Knee-X-ray')\n",
        "        found_data_dir = True\n",
        "        break\n",
        "\n",
        "if not found_data_dir:\n",
        "    print(f\"Error: Could not find 'normal', 'osteopenia', 'osteoporosis' directories within {FINAL_DATA_EXTRACT_DIR}.\")\n",
        "    # در این حالت، بهتر است محتویات FINAL_DATA_EXTRACT_DIR را لیست کنیم تا کاربر ببیند مشکل کجاست.\n",
        "    print(\"\\n--- Contents of FINAL_DATA_EXTRACT_DIR ---\")\n",
        "    !ls -R {FINAL_DATA_EXTRACT_DIR}\n",
        "    raise FileNotFoundError(\"Class directories not found. Please verify the dataset structure after extraction.\")\n",
        "\n",
        "\n",
        "print(f\"Final BASE_DATA_ROOT set to: {BASE_DATA_ROOT}\")\n",
        "\n",
        "\n",
        "# --- ۳. تعریف نام کلاس‌ها و جمع‌آوری تمام مسیرهای فایل‌ها و لیبل‌های اصلی ---\n",
        "# نام‌های کلاس‌ها مطابق با ساختار پوشه‌هایی که شما ارسال کردید.\n",
        "CLASS_FOLDERS = ['normal', 'osteopenia', 'osteoporosis']\n",
        "FINAL_NUM_CLASSES = len(CLASS_FOLDERS) # خروجی مدل: 3 کلاس\n",
        "\n",
        "all_filepaths = []\n",
        "all_original_labels = []\n",
        "\n",
        "print(f\"\\nCollecting images from: {BASE_DATA_ROOT} with classes: {CLASS_FOLDERS}\")\n",
        "for class_name in CLASS_FOLDERS:\n",
        "    current_class_path = os.path.join(BASE_DATA_ROOT, class_name)\n",
        "    if not os.path.exists(current_class_path):\n",
        "        print(f\"Warning: Class directory '{current_class_path}' not found. Skipping.\")\n",
        "        continue # اگر پوشه ای نبود، ردش می‌کنیم\n",
        "\n",
        "    for img_name in os.listdir(current_class_path):\n",
        "        if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "            all_filepaths.append(os.path.join(current_class_path, img_name))\n",
        "            all_original_labels.append(class_name)\n",
        "\n",
        "print(f\"Found {len(all_filepaths)} total images for classes: {np.unique(all_original_labels)}.\")\n",
        "\n",
        "# --- ۴. تعریف نگاشت کلاس‌ها (اختیاری، اما برای class_weight مفید است) ---\n",
        "# flow_from_directory به صورت خودکار mapping را بر اساس نام پوشه انجام می‌دهد.\n",
        "# اما برای class_weight باید mapping عددی داشته باشیم.\n",
        "unique_sorted_labels = sorted(np.unique(all_original_labels))\n",
        "CLASS_TO_INT_MAPPING = {label: i for i, label in enumerate(unique_sorted_labels)}\n",
        "INT_TO_CLASS_MAPPING = {i: label for i, label in enumerate(unique_sorted_labels)}\n",
        "\n",
        "print(f\"Class to integer mapping: {CLASS_TO_INT_MAPPING}\")\n",
        "\n",
        "\n",
        "# --- ۵. تنظیم هایپرپارامترها ---\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "TEST_SPLIT_RATIO = 0.2\n",
        "VALIDATION_SPLIT_RATIO = 0.1\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# --- تغییر نام متغیرهای اپوک و نرخ یادگیری برای مدل‌ها ---\n",
        "EPOCHS_ML_MODEL_1 = 20 # نام‌گذاری جدید\n",
        "EPOCHS_ML_MODEL_2 = 15 # نام‌گذاری جدید\n",
        "\n",
        "LEARNING_RATE_ML_MODEL_1 = 0.001 # نام‌گذاری جدید\n",
        "LEARNING_RATE_ML_MODEL_2 = 0.0001 # نام‌گذاری جدید\n",
        "\n",
        "# --- ۶. آماده‌سازی ساختار موقت برای ImageDataGenerator ---\n",
        "TEMP_SPLIT_DIR = \"/content/temp_dataset_split\"\n",
        "TRAIN_TEMP_DIR = os.path.join(TEMP_SPLIT_DIR, 'train')\n",
        "VAL_TEMP_DIR = os.path.join(TEMP_SPLIT_DIR, 'validation')\n",
        "TEST_TEMP_DIR = os.path.join(TEMP_SPLIT_DIR, 'test')\n",
        "\n",
        "if os.path.exists(TEMP_SPLIT_DIR):\n",
        "    shutil.rmtree(TEMP_SPLIT_DIR)\n",
        "os.makedirs(TRAIN_TEMP_DIR, exist_ok=True)\n",
        "os.makedirs(VAL_TEMP_DIR, exist_ok=True) # این پوشه ها در نهایت توسط ImageDataGenerator ایجاد می‌شوند\n",
        "os.makedirs(TEST_TEMP_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "df_full = pd.DataFrame({'filepath': all_filepaths, 'original_label': all_original_labels})\n",
        "# اضافه کردن ستون برای لیبل‌های عددی\n",
        "df_full['int_label'] = df_full['original_label'].map(CLASS_TO_INT_MAPPING)\n",
        "\n",
        "\n",
        "df_train_val, df_test = train_test_split(\n",
        "    df_full, test_size=TEST_SPLIT_RATIO, random_state=RANDOM_SEED, stratify=df_full['int_label']\n",
        ")\n",
        "\n",
        "def populate_temp_dirs(dataframe, base_target_dir):\n",
        "    # استفاده از tqdm برای نمایش نوار پیشرفت کپی کردن فایل‌ها\n",
        "    for _, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=f\"Copying to {os.path.basename(base_target_dir)}\"):\n",
        "        original_path = row['filepath']\n",
        "        # از لیبل اصلی (نام پوشه) استفاده می‌کنیم، چون flow_from_directory به آن نیاز دارد\n",
        "        class_folder_name = row['original_label']\n",
        "\n",
        "        target_dir = os.path.join(base_target_dir, class_folder_name)\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        target_file = os.path.join(target_dir, os.path.basename(original_path))\n",
        "        if not os.path.exists(target_file):\n",
        "            shutil.copy(original_path, target_file)\n",
        "\n",
        "print(\"\\nPopulating temporary train/test directories...\")\n",
        "populate_temp_dirs(df_train_val, TRAIN_TEMP_DIR)\n",
        "populate_temp_dirs(df_test, TEST_TEMP_DIR)\n",
        "print(\"Temporary directories populated.\")\n",
        "\n",
        "# --- ۷. تعریف تبدیل‌ها (Data Augmentation) و ImageDataGenerator ---\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    validation_split=VALIDATION_SPLIT_RATIO\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_TEMP_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # برای دسته‌بندی چندکلاسه\n",
        "    subset='training',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_TEMP_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # برای دسته‌بندی چندکلاسه\n",
        "    subset='validation',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    TEST_TEMP_DIR,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # برای دسته‌بندی چندکلاسه\n",
        "    shuffle=False # برای ارزیابی نهایی، shuffle نباید باشد\n",
        ")\n",
        "\n",
        "# محاسبه class_weights برای مقابله با عدم تعادل کلاس‌ها\n",
        "unique_classes_for_weight = np.unique(df_train_val['int_label'])\n",
        "calculated_class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=unique_classes_for_weight,\n",
        "    y=df_train_val['int_label']\n",
        ")\n",
        "class_weights_dict = {i: calculated_class_weights[i] for i in range(len(unique_classes_for_weight))}\n",
        "print(f\"Calculated class weights for training: {class_weights_dict}\")\n",
        "\n",
        "# --- ۸. تعریف مدل CNN سفارشی از پایه (ML Model 1) ---\n",
        "def build_custom_cnn(input_shape, num_classes):\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "input_shape = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "model_ml1 = build_custom_cnn(input_shape, FINAL_NUM_CLASSES) # تغییر نام\n",
        "model_ml1.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_ML_MODEL_1), # تغییر نام\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- ML Model 1 (Custom CNN) ---\") # تغییر نام\n",
        "model_ml1.summary()\n",
        "\n",
        "# --- ۹. تعریف مدل یادگیری انتقالی (ML Model 2 - ResNet50) ---\n",
        "def build_transfer_model(input_shape, num_classes):\n",
        "    base_model = keras.applications.ResNet50(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model_ml2 = build_transfer_model(input_shape, FINAL_NUM_CLASSES) # تغییر نام\n",
        "model_ml2.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_ML_MODEL_2), # تغییر نام\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "print(\"\\n--- ML Model 2 (ResNet50) ---\") # تغییر نام\n",
        "model_ml2.summary()\n",
        "\n",
        "\n",
        "# --- ۱۰. آموزش و ارزیابی مدل‌ها ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- شروع آموزش ML Model 1 ---\") # تغییر نام\n",
        "print(\"=\"*50)\n",
        "history_ml1 = model_ml1.fit( # تغییر نام\n",
        "    train_generator,\n",
        "    epochs=EPOCHS_ML_MODEL_1, # تغییر نام\n",
        "    validation_data=validation_generator,\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- شروع آموزش ML Model 2 ---\") # تغییر نام\n",
        "print(\"=\"*50)\n",
        "history_ml2 = model_ml2.fit( # تغییر نام\n",
        "    train_generator,\n",
        "    epochs=EPOCHS_ML_MODEL_2, # تغییر نام\n",
        "    validation_data=validation_generator,\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "# --- ۱۱. ارزیابی نهایی و نمایش نتایج در یک جدول ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- نتایج نهایی ارزیابی روی مجموعه تست ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "target_names_for_report = [INT_TO_CLASS_MAPPING[i] for i in sorted(INT_TO_CLASS_MAPPING.keys())]\n",
        "\n",
        "# پیش‌بینی‌ها برای ML Model 1\n",
        "y_pred_ml1_probs = model_ml1.predict(test_generator) # تغییر نام\n",
        "y_pred_ml1 = np.argmax(y_pred_ml1_probs, axis=1)\n",
        "y_true_test_indices = test_generator.classes\n",
        "\n",
        "ml1_acc = accuracy_score(y_true_test_indices, y_pred_ml1) # تغییر نام\n",
        "ml1_prec = precision_score(y_true_test_indices, y_pred_ml1, average='weighted', zero_division=0) # تغییر نام\n",
        "ml1_rec = recall_score(y_true_test_indices, y_pred_ml1, average='weighted', zero_division=0) # تغییر نام\n",
        "ml1_f1 = f1_score(y_true_test_indices, y_pred_ml1, average='weighted', zero_division=0) # تغییر نام\n",
        "\n",
        "print(f\"ML Model 1 -> Accuracy: {ml1_acc:.4f}, Precision: {ml1_prec:.4f}, Recall: {ml1_rec:.4f}, F1-score: {ml1_f1:.4f}\") # تغییر نام\n",
        "print(\"\\nClassification Report for ML Model 1:\") # تغییر نام\n",
        "print(classification_report(y_true_test_indices, y_pred_ml1, target_names=target_names_for_report, zero_division=0))\n",
        "print(\"Confusion Matrix for ML Model 1:\") # تغییر نام\n",
        "print(confusion_matrix(y_true_test_indices, y_pred_ml1))\n",
        "\n",
        "\n",
        "# پیش‌بینی‌ها برای ML Model 2\n",
        "y_pred_ml2_probs = model_ml2.predict(test_generator) # تغییر نام\n",
        "y_pred_ml2 = np.argmax(y_pred_ml2_probs, axis=1)\n",
        "\n",
        "ml2_acc = accuracy_score(y_true_test_indices, y_pred_ml2) # تغییر نام\n",
        "ml2_prec = precision_score(y_true_test_indices, y_pred_ml2, average='weighted', zero_division=0) # تغییر نام\n",
        "ml2_rec = recall_score(y_true_test_indices, y_pred_ml2, average='weighted', zero_division=0) # تغییر نام\n",
        "ml2_f1 = f1_score(y_true_test_indices, y_pred_ml2, average='weighted', zero_division=0) # تغییر نام\n",
        "\n",
        "print(f\"\\nML Model 2 -> Accuracy: {ml2_acc:.4f}, Precision: {ml2_prec:.4f}, Recall: {ml2_rec:.4f}, F1-score: {ml2_f1:.4f}\") # تغییر نام\n",
        "print(\"\\nClassification Report for ML Model 2:\") # تغییر نام\n",
        "print(classification_report(y_true_test_indices, y_pred_ml2, target_names=target_names_for_report, zero_division=0))\n",
        "print(\"Confusion Matrix for ML Model 2:\") # تغییر نام\n",
        "print(confusion_matrix(y_true_test_indices, y_pred_ml2))\n",
        "\n",
        "\n",
        "data = {\n",
        "    'Model': ['ML Model 1', 'ML Model 2'], # تغییر نام در جدول\n",
        "    'Accuracy': [ml1_acc, ml2_acc],\n",
        "    'Precision': [ml1_prec, ml2_prec],\n",
        "    'Recall': [ml1_rec, ml2_rec],\n",
        "    'F1-score': [ml1_f1, ml2_f1]\n",
        "}\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "print(\"\\n--- جدول نتایج نهایی مقایسه مدل‌ها ---\")\n",
        "print(df_results.round(4).to_markdown(index=False))\n",
        "\n",
        "\n",
        "# --- ۱۲. رسم نمودار Loss و Accuracy در طول آموزش ---\n",
        "def plot_history(history, model_name):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue', marker='o', linestyle='--', markersize=4)\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', marker='x', linestyle='-', markersize=4)\n",
        "    plt.title(f'{model_name} - Accuracy over Epochs', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Accuracy', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss', color='green', marker='o', linestyle='--', markersize=4)\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', color='purple', marker='x', linestyle='-', markersize=4)\n",
        "    plt.title(f'{model_name} - Loss over Epochs', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, linestyle=':', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- رسم نمودارهای آموزش ---\")\n",
        "plot_history(history_ml1, \"ML Model 1\") # تغییر نام\n",
        "plot_history(history_ml2, \"ML Model 2\") # تغییر نام\n",
        "\n",
        "# --- ۱۳. پاکسازی پوشه‌های موقت ---\n",
        "print(\"\\n--- Cleaning up temporary directories and downloaded files ---\")\n",
        "if os.path.exists(FIRST_EXTRACT_DIR):\n",
        "    shutil.rmtree(FIRST_EXTRACT_DIR)\n",
        "    print(f\"Removed first extraction directory: '{FIRST_EXTRACT_DIR}'.\")\n",
        "if os.path.exists(FINAL_DATA_EXTRACT_DIR) and FINAL_DATA_EXTRACT_DIR != FIRST_EXTRACT_DIR:\n",
        "    shutil.rmtree(FINAL_DATA_EXTRACT_DIR)\n",
        "    print(f\"Removed final dataset extraction directory: '{FINAL_DATA_EXTRACT_DIR}'.\")\n",
        "if os.path.exists(TEMP_SPLIT_DIR):\n",
        "    shutil.rmtree(TEMP_SPLIT_DIR)\n",
        "    print(f\"Removed temporary data split directory: '{TEMP_SPLIT_DIR}'.\")\n",
        "if os.path.exists(OUTER_ZIP_FILE_NAME):\n",
        "    os.remove(OUTER_ZIP_FILE_NAME)\n",
        "    print(f\"Removed downloaded outer zip file: '{OUTER_ZIP_FILE_NAME}'.\")\n",
        "if INNER_ZIP_FILE_PATH and os.path.exists(INNER_ZIP_FILE_PATH):\n",
        "    # این خط فقط اگر زیپ داخلی در یک مسیر جداگانه باقی مانده باشد آن را حذف می‌کند\n",
        "    # (نه اگر بخشی از محتوای FIRST_EXTRACT_DIR باشد که خودش حذف می‌شود)\n",
        "    if not INNER_ZIP_FILE_PATH.startswith(FIRST_EXTRACT_DIR) or not os.path.exists(FIRST_EXTRACT_DIR):\n",
        "        os.remove(INNER_ZIP_FILE_PATH)\n",
        "        print(f\"Removed inner zip file: '{INNER_ZIP_FILE_PATH}'.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"--- پایان اجرای برنامه ---\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "hoVNDaL3mojw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}